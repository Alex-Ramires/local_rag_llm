{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import langchain\n",
    "\n",
    "# LLM\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Document loader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "# from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "# Text splitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Embedding\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "\n",
    "# Vector store\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "\n",
    "# Prompt\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "MODEL = 'llama3.1'\n",
    "DATA_PATH = 'data'\n",
    "CHROMA_PATH = 'chroma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ollama(model = MODEL)\n",
    "# model.invoke('Olá, tudo bem?')\n",
    "\n",
    "embeddings = OllamaEmbeddings(model = 'nomic-embed-text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and chunking the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for filename in os.listdir(DATA_PATH):\n",
    "    if filename.endswith('.pdf'):\n",
    "        file_path = os.path.join(DATA_PATH, filename)\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents.extend(loader.load())\n",
    "        \n",
    "text_spitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap = 20,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False\n",
    ")\n",
    "\n",
    "chunks = text_spitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "db = Chroma.from_documents(\n",
    "    documents = chunks,    \n",
    "    embedding = embeddings,\n",
    "    persist_directory = CHROMA_PATH\n",
    ")\n",
    "\n",
    "db.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LLama with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "ALWAYS return a \"SOURCES\" part in your answer. Answer in Portuguese.\n",
    "\n",
    "QUESTION: {question}\n",
    "=========\n",
    "{summaries}\n",
    "=========\n",
    "FINAL ANSWER:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template = prompt_template, input_variables = [\"summaries\", \"question\"]\n",
    ")\n",
    "\n",
    "langchain.verbose = False\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm = model,\n",
    "    retriever = db.as_retriever(),\n",
    "    chain_type_kwargs={\n",
    "        'prompt': PROMPT,\n",
    "        'document_variable_name': 'summaries'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = 'Eu posso receber propina?'\n",
    "response = chain.invoke(QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pergunta: Eu posso receber propina?\n",
      "\n",
      "\n",
      "Resposta: Desculpe, mas não posso responder a essa pergunta. \n",
      "\n",
      "SOURCES:\n",
      "\n",
      "* Não há informações no texto fornecido que indiquem se é permitido receber propina ou pagamentos indevidos. \n",
      "* O texto parece enfatizar a importância de uma relação transparente e ética com fornecedores, mas não aborda especificamente o recebimento de propina.\n"
     ]
    }
   ],
   "source": [
    "resposta = response['result']\n",
    "\n",
    "print(f'Pergunta: {QUERY}')\n",
    "print('\\n')\n",
    "print(f'Resposta: {resposta}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
